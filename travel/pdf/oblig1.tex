\documentclass[10pt, a4paper]{article}

\usepackage[]{graphicx}
\usepackage{subcaption}
\usepackage[]{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}
\usepackage{ gensymb }
\usepackage{parskip}
\definecolor{red}{rgb}{1,0,0}
\definecolor{green}{rgb}{0,1,0}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{blue},
    numberstyle=\tiny\color{codepurple},
    stringstyle=\color{codegreen},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}
\usepackage[utf8]{inputenc}

\lstset{
% 	frame = single,
% 	language = R,
% 	showstringspaces = false,
% 	tabsize = 2,
	otherkeywords = {self},
	keywordstyle = \color{magenta},
	identifierstyle=\color{black},
%	numberstyle = \color{yellow},
% 	backgroundcolor=\color{backcolour}
}

\title{Oblig 1 The Traveling Salesman Problem \\
  \hrulefill\small{ INF4490 }\hrulefill}
  
\author{Joseph Knutson \\
\href{https://github.com/mathhat/}{\texttt{github.com/mathhat}}}
  
\begin{document}

\maketitle
\tableofcontents
\clearpage
\section{Exhaustive search}
This section guides you through the code, efficiency and result of the exhaustive search method for the traveling salesman problem.
\subsection{Creating the Code}
Making my Exhaustive Search code began with using the example.py file on the course site which imports the city grid.
From there I followed the advice of the assignment regarding the itertools module's permutations function. Looping over every sequence and summing the distances for each sequence, the final Exhaustive search function looked something like this:
\begin{lstlisting}[language = Python]
  start = time.time()             #start clock
  for sequence in Permutations:   #exhaustive search begins
      dist = 0
      for index in range(cities-1):
	        dist += distances[sequence[index]][sequence[index+1]]
      dist += distances[sequence[cities-1]][sequence[0]]
      if dist < best:             #save shortest distance yet
	        best=dist
	        best_sequence = sequence

  end = time.time()              #end clock
  Time = (end-start)             #sum time
  return(best, best_sequence, Time)
\end{lstlisting}
You can observe on the last line that the function returns the shortest path's distance, the path sequence and the time it took to iterate over all the permutations.
\subsection{Timetables}
The time it takes for the program to run varies with the amount of cities we add. However, it is not sufficiently accurate to measure only once.
Calling the Exhaustive Search function 10 times helped create an approximate time average for its execution time, and I did so for the first 6 to 10 cities in the european\_cities.csv file. 
The result can be seen in figure \ref{fig:time_exhaust}. The code calculating the timeaverages and producing the plot lies in the exhaustive\_time.py file.

\begin{figure}[h!]\centering
\includegraphics[width = \linewidth]{time_exhaust.png} 
\caption{Time to calculate shortest path as a function of the number of cities.}
\label{fig:time_exhaust}
\end{figure}

If you go into the time\_exhaustive.txt file (created by running exhaustive\_time.py), you can quickly find how long the calculations took (below). Let's use these numbers to predict how long it takes to use Exhaustive Search with 24 cities.
\begin{lstlisting}
Time average for 7 cities = 0.005085 seconds 

Time average for 8 cities = 0.042169 seconds 

Time average for 9 cities = 0.420994 seconds 

Time average for 10 cities = 4.878382 seconds 
\end{lstlisting}
To find out how long simulations of a higher city number will take, we try to make a model based on the amount of flops (+ - * /) that are executed.
The number of permutations for 10 cities are equal to N!$ = 3628800$ and for each permutation (or path) one has to add the distances between each city to calculate the total distance of the path. 
This leaves us with N$*$N!$ = 36288000$ floating point operations (flops). If we divide the amount of flops the program used on the runtime of the program, we get an approximation of how many flops (additions) occur per second:
$$\text{flops\_per\_sec} = \frac{36288000}{4.878382 \text{sec}} = 7438531.87389 \text{flops/sec}$$

This approximation of a constant can give us an idea of how long it takes to run an exhaustive search for a larger amount of cities.
To find the runtime of a larger path with 24 cities, we need to find the amount of flops the program will require (N$*$N!):
$$\text{flops(24 cities)} = 24*24! = 1.4890762\ 10^{25} \text{flops}$$
Now we can calculate the seconds this program will need to finish running (not to mention the insane amount of RAM):
$$\text{runtime of ES for 24 cities} = \frac{1.4890762\ 10^{25} \text{flops}}{\text{flops\_per\_sec}} \approx 63.4\ 10^{9} \text{years} $$
The calculations make it obvious that Exhaustive Search is useless once the amount of cities pass 10.
\subsection{Shortest Path Result}

Our Exhaustive Search function returns not only time, but also the distance, \emph{best}, and the sequence of cities traveled along the shortest path, \emph{sequence}.
\begin{lstlisting}
'Best' = ', 7486.309999999999
'Best sequence = ', (6, 8, 3, 7, 0, 1, 9, 4, 5, 2)

Barcelona, Belgrade, Berlin, Brussels, Bucharest, Budapest, Copenhagen, Dublin, Hamburg, Istanbul
\end{lstlisting}
Above is the result for $n_{cities} = 10$. I've written the cities in alphabetic order. The sequence has translated each city into a number from 0 to 9. 
If we place the 10 cities' names in the order that creates the shortest path, we get: 
\begin{lstlisting}
Copenhagen Hamburg Brussels Dublin Barcelona Belgrade Istanbul Bucharest Budapest Berlin 
\end{lstlisting}
\clearpage
\section{Hillclimber}
This section compares my Hillclimber method with the Exhaustive Search method from the previous assignment. The method I've written is heavily inspired by the course book's (\emph{Machine Learning}) example.
The Hillclimber method tries to find a maxima or a minima of a fitness function by slightly tweaking the parameters of the problem in random ways, for our problem the tweaks involve reordering the sequence of cities we travel to.
Like the example in the book, I pick any two random cities in an initially created path and switch their order of travel.
\subsection{Code}
\begin{lstlisting}[language = Python]
#Inspired by the book example

def hillclimb(distances, n_cities,seed):
    np.random.seed(seed)
    #updating seed gives different initial sequences per run

				      #order of cities we visit 
    sequence = np.asarray(range(n_cities)) 

    np.random.shuffle(sequence)  #create an initial path, from 
                                 #this order make small changes
    
    distanceTravelled = np.inf   #variable updated to shortest path

    i = 0                    #loop variable to signify 1000 changes

    while i < 1000:
        newDist = 0 
	
        #declaring variable to compare a new-
        #path to the previously shortest path.
       
        #Choose 2 random integers representing cities and change  
        #the initial path by switching/reordering their position
        city1 = np.random.randint(n_cities)     
        city2 = np.random.randint(n_cities) 
        
        if city1 != city2:                      
            i += 1              

            #If the cities are not the same, the we switch
            #their position and count this  hillclimbing operation
            
            posSeq = sequence.copy()
            posSeq = np.where(posSeq==city1,-1, posSeq)
            posSeq = np.where(posSeq==city2,city1, posSeq)
            posSeq = np.where(posSeq==-1,city2, posSeq)
            
            #Here I simply sum up the distance of the path like in exhaustive search
            
            for j in range(n_cities-1): 
                newDist += distances[posSeq[j]][posSeq[j+1]]
            newDist += distances[posSeq[-1]][posSeq[0]]
            
            #Now we can compare the old distance with the new path
            #- created by the hillclimbing operation above
            
            if newDist < distanceTravelled: 
                distanceTravelled = newDist
                sequence = posSeq
    return sequence, distanceTravelled          #returns both path distance and which order the cities are traveled to
\end{lstlisting}

\subsection{Results}
For the hillclimbing method, we measured the distance traveled instead of the time it takes to run the program (almost instantaneous).
As you can see from the while loop in the code above, I use the hillclimbing operator 1000 times for each run, and I run the program 20 times for both $$\text{n\_cities} = 10 \text{ and } \text{n\_cities} = 24.$$
This produces a heap of results from which we can pick the longest distance (worst result) and the shortest distance (solution) and even calculate the standard deviation.

To do distance measurements and write them to file i simply import the hillclimber function and call it 20 times.

The following code can be found in \emph{hillclimbing\_dist.py} and the results are precalculated
in the text files: \emph{dist\_hillclimber10cities.txt} and \emph{dist\_hillclimber10cities.txt}.
\begin{lstlisting}[language = Python]
 for i in range(n_sims):                          #n_sims = 20
    seq, dist =  hillclimb(distances, n_cities,i) #i is also used as a seed
    lengths[i]=dist

lengths=sorted(lengths) #sorting the results

File = open("dist_hillclimber%scities.txt" % n_cities, "w")   

for i in range(len(lengths)):
    File.write("%.2f"%lengths[i])
    File.write("\n"%lengths[i])

File.write("\n"%lengths[i])
File.write("standard dev = %.2f"%standard_dev)
\end{lstlisting}
After writing the distances to file, we can print them in the terminal:

\begin{lstlisting}
$ cat dist_hillclimber10cities.txt 
7486.31
7486.31
7486.31
7486.31
7503.10
7503.10
7503.10
7503.10
7503.10
7503.10
7503.10
7737.95
7737.95
7737.95
7737.95
7737.95
7737.95
7737.95
8349.94
8349.94

standard dev = 209.85

$ cat dist_hillclimber24cities.txt 
13456.51
13650.46
13665.88
13806.32
13817.47
14119.10
14190.63
14209.14
14305.25
14314.38
14394.55
14410.91
14665.60
15329.68
15575.97
15695.83
16062.43
16256.35
16317.54
16381.76

standard dev = 961.10
\end{lstlisting}
The standard deviation from these results can be expressed as $$ \sigma = \sqrt{ \left[ E(x^2) - (E(x))^2 \right] }$$
where E is the mean operator, x is the array of the distance of the paths found and x$^2$ is an array of these distances squared.
Implemented, the standard deviation of our results look like this:
\begin{lstlisting}[language = Python]
Mean = np.mean(lengths)
Mean_sq = np.mean(lengths*lengths)
standard_dev = np.sqrt(Mean_sq-Mean**2)
\end{lstlisting}
These expressions return the standard deviation values: \\
$\sigma(\text{n\_sims} = 20, \text{ n\_cities} = 10) = 209.85$ and \\
$\sigma(\text{n\_sims} = 20, \text{ n\_cities} = 24) = 961.10$ \\

One unique seed is used for each of the 20 simulations/hillclimb calls, from \\seed $= 1$ to seed $= 20$.

Compared to the exhaustive search method, we easily solve the n\_city $= 10$ problem by achieving a distance of 7486.31.
We need a smarter algorithm, or more than 20 unique initial paths to solve the problem for all 24 cities when using hillclimbing, however.
Since I've implemented a hillclimber which only has 1 populant and 1 offspring, I am stuck in a local maximum.

\section{Genetic Algorithm}
To solve the TSP, we're going to use partially mapped crossover with two parents and one offspring.
After creating offspring by using the PMX operations as many times as there are parents, I am left with
as many offspring as parents. From the offspring and parents, I choose elitism, and only pick the fittest.
This defines 1 generation, but since my last delivery I now run 10 generations per run instead of 1.
The population is held constant and my 3 tested population values are 10, 100 and 1000.
\subsection{PMX code}
The partially\_mapped function defines most of what happens in my ga.py script, but the function is found in functions.py: this is how it looks
\begin{lstlisting}[language = Python]
 
def partially_mapped(parents, distances, n_cities, n_pop):
    
    offspring = np.zeros((n_pop,n_cities),int)    #matrix n_population x n_cities

    for iterations in range(n_pop):
        #Choosing parents to mate
        parent12 = np.random.randint(0,n_pop,2,int)
        parent1 = min(parent12)
        parent2 = max(parent12)
        index1 = np.random.randint(0,n_cities)
        index2 = index1+2
        if (index1 != index2) and (parent1 != parent2) :  #Making sure parents are different and -
            #Here starts the partially mapped crosseover

            #initial kids are identical to parents

            offspring[iterations] = parents[parent1]


            #crossover genomes/sequences
            genome1 = parents[parent1][index1:index2]
            genome2 = parents[parent2][index1:index2]

            #inserting genomes
            offspring[iterations][index1:index2] = genome2 
            #offspring 1 gets sequence from parent 2
            #crossover from parents to offspring:
            for i in range(len(genome2)):
                if genome1[i] in genome2: 
                    #instance where gene in crossover 
                    #- sequence must be ignored
                    pass
                else:
                    #Here the fun pinball dynamics take place
                    gene = genome2[i]
                    success = 0
                    while success == False:
                        if gene == genome2[np.where(genome1==gene)]:
                            success = True
                            pass
                        if gene in genome1: #This thing removes infinite loops
                            gene = genome2[np.where(genome1==gene)]
                        else:
                            offspring[iterations][np.where(parents[parent1]==gene)] = genome1[i]
                            success = True
    return offspring
    \end{lstlisting}
After the code creates offspring n\_pop times by crossing over parents, the offspring is returned, assembled with the parents, sorted and then checked for elimination. 
My sorting function takes a matrix filled with the parents and offspring combined and arranges the solutions by shortest path length:
\begin{lstlisting}[language = Python]
 def sort(pop_matrix,distances): 
    population = []     #best solutions
    path_lengths = []   #way of finding best solutions
    pop_matrix = pop_matrix[0] 

    n_cities = len(pop_matrix[0]) #city number
    for sequence in pop_matrix:   #first calculate path lenths
        dist = 0
        if sum(sequence)>0:       #ignores sequences like [0,0,0,0,0]
            for index in range(n_cities-1):
                dist += distances[sequence[index]][sequence[index+1]]
            dist += distances[sequence[n_cities-1]][sequence[0]]
            population.append(sequence)
            path_lengths.append(dist)

    population = np.asarray(population)
    path_lengths = np.asarray(path_lengths)

    ranked_paths = []
    ranked_sequences = []
    for i in range(len(population)): #then arrange them in diminishing order
        ranked_paths.append(np.amin(path_lengths))
        ranked_sequences.append(population[np.argmin(path_lengths)])
        path_lengths = np.delete(path_lengths, np.argmin(path_lengths)) 
        
    return np.asarray(ranked_paths), np.asarray(ranked_sequences) 
\end{lstlisting}



The population is held static by elimination. The mating is then resumed for another generation to go by. In my program \emph{ga.py} the number of generations per run is set to 10.
This means n\_pop pmx operations followed by 1 filtering will happen 10 times per run.
\subsection{Results}
By running my ga.py script, three files are produced which return all sequences and their lengths in an ordered fashion. These files are already produced in my folder and possess names like \emph{GA\_result\_of\_10cities\_and\_1000populants.txt}:
\\
\begin{lstlisting}
These are the results for a poplation of size 1000 who 20 times have jumped 10 generations.
For each generation, 1000 PMX operations create as many offspring as parents.
After each 1000 PMX operation, an elitist filter is applied to keep the population static and only the best solutions available. 
The sequences are printed to terminal since I'm having trouble writing them to file.
path lengths in increasing order: 
path lengths    sequences: 
  7486.31        9452683701  #1st/best
  7486.31        9783102546
  7486.31        8127039465
  7486.31        8179345260
  7486.31        3125798046
  7486.31        1073862549
  7663.51        9452687301
  7663.70        9452863701
  7729.01        9452867301
  7775.10        3086795241
	    [...]
  12818.33        9841052673 #1000th / worst
\end{lstlisting}
Here's the final result of the GA using 24 cities and 1000 populants with 10 generations per run, for 20 runs. \emph{GA\_result\_of\_24cities\_and\_1000populants.txt}:

\begin{lstlisting}
These are the results for a poplation of size 1000 who 20 times have jumped 10 generations.
[...] path lengths in increasing order: 
13915.25 #1st
13915.25
14746.64
14746.64
16138.36
16138.36
16138.36
16138.36
16138.36
[...]
30347.27 #1000th
\end{lstlisting}

From these results we can see that the best solution for the TSP is reachable.


\begin{figure}[h!]\centering
\includegraphics[width = \linewidth]{fitness24} 
\caption{Indeed, we see a diminishing average path length for the population for each run of the GA algorithm.}
\label{fig:time_exhaust}
\end{figure}

\clearpage
\section{Results Across Methods}
As I've understood from the assignment, I am to pick the best individuals of the population for every run (20) and present the best, worst and their standard deviation.
For low populations, the best individual tends to stay the same, which is why the standard deviation goes towards zero. 




Here's data from the GA:
\begin{lstlisting}
10 cities, 10 populants
('best of the 20 individuals', 9737.3999999999996)
('worst of the 20 individuals', 9737.3999999999996)
('standard dev of 20 best individuals', 0.00017263349150062197)
10 cities, 100 populants
('best of the 20 individuals', 7503.1000000000004)
('worst of the 20 individuals', 7745.8000000000011)
('standard dev of 20 best individuals', 52.89523867990529)
10 cities, 1000 populants
('best of the 20 individuals', 7486.3099999999995)
('worst of the 20 individuals', 7767.1600000000008)
('standard dev of 20 best individuals', 59.641132207667056)
  
 
24 cities, 10 populants
('best of the 20 individuals', 27431.400000000005)
('worst of the 20 individuals', 27431.400000000005)
('standard dev of 20 best individuals', 0.00034526698300124393)
24 cities, 100 populants
('best of the 20 individuals', 18899.630000000001)
('worst of the 20 individuals', 21879.710000000003)
('standard dev of 20 best individuals', 750.81412341537668)
24 cities, 1000 populants
('best of the 20 individuals', 13915.25)
('worst of the 20 individuals', 21663.470000000001)
('standard dev of 20 best individuals', 2109.1238597016604)
\end{lstlisting}
From what we see here, a larger population tend to sprout the best individuals, but the standard deviation increases
since the worst individuals differ largely from the best.

The best solution for the TSP is found for 10 cities, but not for 24 with the GA. The Hillclimber, however, does slightly better than the GA on the 24 city problem and uses less mutations/operations to do so!

Timewise, the GA uses less than 1 second for populations below 100 populants, but for 1000 (which seems necessary for the best solutions), the script uses about 13 seconds, while exhaustive search uses 5 seconds.
This means that GA is a good idea when you have more than 10 cities to go to, but for 10 cities, the exhaustive search is a good idea.

All in all, the hillclimber is the fastest and its solution for 24 cities is better than the GA. As you probably know, exhaustive search is not applicable to the 24 city problem.
Hillclimber wins (perhaps due to a bad GA on my side).
\clearpage
\section{Hybrid}
Unable to find a proper definition for Balwinian learning online, I believe I must settle for a Lamarckian method where I for each run mutate my population with the hillclimber.
This mutation will help genetic variation as well as average fitness, since my hillclimber always returns a better individual.


\subsection{Results}
In hybrid.py I've copied my GA script, but at the end of each run, the entire population is mutated will a single hillclimbing operation, and if the offspring is better than the original individual, the offspring takes the place of its parent.
Below is the returned values from running the hybrid.py script.
\begin{lstlisting}
10 cities, 10 populants
('best of the 20 individuals', 7486.3100000000004)
('worst of the 20 individuals', 12893.459999999999)
('standard dev of 20 best individuals', 1654.7905677257222)
10 cities, 100 populants
('best of the 20 individuals', 7549.1599999999999)
('worst of the 20 individuals', 12741.6)
('standard dev of 20 best individuals', 1544.5079162030734)
10 cities, 1000 populants
('best of the 20 individuals', 7486.3099999999995)
('worst of the 20 individuals', 12985.280000000001)
('standard dev of 20 best individuals', 1324.8932242520507)

24 cities, 10 populants
('best of the 20 individuals', 16445.420000000002)
('worst of the 20 individuals', 27191.360000000008)
('standard dev of 20 best individuals', 2859.7024777331267)
24 cities, 100 populants
('best of the 20 individuals', 14773.569999999998)
('worst of the 20 individuals', 21609.950000000001)
('standard dev of 20 best individuals', 2077.5917025485683)
24 cities, 1000 populants
('best of the 20 individuals', 16489.220000000001)
('worst of the 20 individuals', 25019.879999999997)
('standard dev of 20 best individuals', 2200.0371211447409)
\end{lstlisting}
It seems the 10 city problem is solved, but as for the 24 city problem, using a lot big population is not improving the solution.
This is quite counter-intuitive to me, and might be the result of a GA or hillclimber not working as it should be. Below you can see the average fitness
decrease, but it happens slower than for the GA. Maybe the fault in my algorithm is blind elitism, or failed attempts to preserve fit parents over unfit offspring.



\begin{figure}[h!]\centering
\includegraphics[width = \linewidth]{fitness242} 
\caption{Hybrid GA with Lamarckian learning is doing better over time, but is worse than pure GA, probably a failure on my side.}
\label{fig:time_exhaust}
\end{figure}

\end{document}